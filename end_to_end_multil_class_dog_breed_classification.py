# -*- coding: utf-8 -*-
"""End-to-end Multil-class Dog Breed Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11vVjmd7wQ4lcUubEpKGKFZZrZz2jLWo2

#End-to-end Multil-class Dog Breed Classification

## Problem
Determining the breed of a dog in an image

## Data
The data is from Kaggle's dog breed identification competition.

https://www.kaggle.com/c/dog-breed-identification/data 

## Evaluation
The evaluation is a file with prediction probabilities for each dog breed of each test image.

* Aim at getting a score in competition that is lesser than 1.
(Score is based on the log loss, so the lesser the better)

https://www.kaggle.com/c/dog-breed-identification/overview/evaluation

## Some information about the data

* Dealing with images (unstructured data) and hence Deep learning approach.
* There are 120 breeds of dogs (120 different classes for softmax unit).
* There are around 10,222 images in the training set (these images have labels).
* There are around 10,357 images in the test set (these images have no labels, the goal is to predict them).

#### Importing libraries
* TensorFlow 2.x 
* TensorFlow Hub 
* Numpy
* Pandas
"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd

print(f'Tensorflow version, {tf.__version__}')
print(f'Tensorflow hub version, {hub.__version__}' )

# Check for GPU availability
print('GPU available' if tf.config.list_physical_devices("GPU") else 'Change runtime to GPU')

"""#### Getting data ready
* Turning data into tensors
"""

labels_csv = pd.read_csv('/content/drive/MyDrive/Dog Vision/labels.csv')
print(labels_csv.describe())
print(labels_csv.head())

# As mentioned there are 120 different dog breeds, checking number of examples for each breed
print(labels_csv['breed'].value_counts().plot.bar(figsize = (25, 5)))

# Viewing an image
from IPython.display import Image
image_name = str(labels_csv['id'][0])
Image(f'/content/drive/MyDrive/Dog Vision/train/{image_name}.jpg')

# Check whether number of images in training set match with the number of labels
import os
if len(os.listdir('/content/drive/MyDrive/Dog Vision/train')) == len(labels_csv['id']):
  print("Match")
else:
  print("Check the dataset again")

# Create pathnames from image ID's
filenames = ["drive/My Drive/Dog Vision/train/" + fname + ".jpg" for fname in labels_csv["id"]]

# Creating a label array
labels = np.array(labels_csv['breed'])

len(filenames) == len(labels)

# Getting the breeds of dogs

breeds = np.unique(labels)
print(f'We have {len(breeds)} number of breeds')

# Turning boolean array into boolean array and then into integer format to utilize it for softmax layer

training_labels = np.array([label == breeds for label in labels], dtype="int")
print(len(training_labels))
print(training_labels[0])

"""#### Preprocessing images 
* Turning images into tensors (RGB values)
"""

# Function for preprocessing

def preprocess_image(image_path, IMG_SIZE=229):
  """
  Input: Image file path, required image_size
  Output: Numeric represent of the image of type tensor (Normalised value)
  """

  # Reading an image file
  image = tf.io.read_file(image_path)
  # Turning into numerical tensor with 3 channels of RGB
  image = tf.image.decode_jpeg(image, channels = 3)
  # Normalizing the values
  image = tf.image.convert_image_dtype(image, tf.float32)
  # Resizing the image
  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])

  return image

def get_image_label(image_path, label):
  """
  INput: Image file path name and the assosciated label
  Output: processes the image and reutrns a tuple of (image, label).
  """
  image = preprocess_image(image_path)
  return image, label

tensor_image = preprocess_image(filenames[0])
print(get_image_label(filenames[0], training_labels[0]))

"""#### Structuring the project
* Train/Dev distribution
"""

X = filenames
y = training_labels

# Setting number of examples(images) to use for experimenting

NUM_IMAGES = 2048 #@param {type:"slider", min:1000, max:10222, step:2}

# Split them into training and validation of total size NUM_IMAGES
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],
                                                  y[:NUM_IMAGES],
                                                  test_size=0.2,
                                                  random_state=42)

len(X_train), len(y_train), len(X_val), len(y_val)

"""#### Creating mini-batches to train the model"""

# A function to create databatches

def get_data_batches(X, y=None, batch_size=32, validation_set = False, test_set = False):
  """
  Takes the data and turnes into mini-batches
  Input: X, y=labels(optional), batch_size(optional), validation_set=boolean, test_set=boolean
  Output: Batches of size `batch_size`
  """

  if test_set:
    print('Batches of test data')
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X)))
    data_batch = data.map(preprocess_image).batch(batch_size)
    return data_batch
  elif validation_set:
    print('Batches of validation data')
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y))) 
    data_batch = data.map(get_image_label).batch(batch_size)
    return data_batch
  else:
    print('Batches of training set')
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y)))
    data = data.shuffle(buffer_size=len(X))
    data_batch = data.map(get_image_label).batch(batch_size)
    return data_batch

# Creating training and validation data batches
train_data = get_data_batches(X_train, y_train)
val_data = get_data_batches(X_val, y_val, validation_set=True)

train_data.element_spec, val_data.element_spec

"""#### Visualizing images with labels

"""

import matplotlib.pyplot as plt

# function for viewing images and labels
def display_image_and_breed(image, label):
  """
  Input: Numerical data of image and label
  Output: Plots a figure with breed as title of the image
  """
  # Setup the figure
  plt.figure(figsize=(5, 5))
  plt.imshow(image)
  # Add the image label as the title
  plt.title(breeds[label.argmax()])
  # Turn the grid lines off
  plt.axis("off")

# Visualizing the data in a training batch
train_images, train_labels = next(train_data.as_numpy_iterator())
display_image_and_breed(train_images[0], train_labels[0])

"""#### Building the model
* Transfer learning
* A deep learning architecture

The URL of the models to try from TensorFlow Hub - 
* https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4
* https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4
* https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/4
"""

# Defining input and output shapes
INPUT_SHAPE = [None, 229, 229, 3] # batch, height, width, colour channels
OUTPUT_SHAPE = len(breeds) #120

# Setup model URL from TensorFlow Hub
MODEL_URL_1 = "https://tfhub.dev/google/imagenet/inception_v3/classification/4"
MODEL_URL_2 = "https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4"
MODEL_URL_3 = "https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/4"

# A function which builds a keras deep learning model
from tensorflow import keras
from tensorflow.keras import layers

def build_model(MODEL_URL, INPUT_SHAPE = INPUT_SHAPE, OUTPUT_SHAPE = OUTPUT_SHAPE):
  print("Building model with:", MODEL_URL)
  model = keras.Sequential(
    [
        hub.KerasLayer(MODEL_URL),
        layers.Dense(512, activation="relu", name="non-freez-1", kernel_regularizer=tf.keras.regularizers.L1(0.01),
        activity_regularizer=tf.keras.regularizers.L2(0.01)),
        layers.Dense(units=OUTPUT_SHAPE, activation="softmax")
    ]
  )

  # Compiling the model

  # Choosing an optimal value from 0.01, 0.001, or 0.0001
  # hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])

  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(from_logits = True),
      optimizer=tf.keras.optimizers.Adam(learning_rate = 0.01), 
      metrics=["accuracy"]
  )

  # Build the model
  model.build(INPUT_SHAPE)

  return model

# build the models

INCEPTION_V3 = build_model(MODEL_URL_1)
INCEPTION_V3.summary()

RESENT_V2 = build_model(MODEL_URL_2)
RESENT_V2.summary()

INCEPTION_RESENT_V2 = build_model(MODEL_URL_3)
INCEPTION_RESENT_V2.summary()

"""#### Creating callbacks

Callbacks are helper functions a model can use during training to do such things as save its progress, check its progress.

* Tensorboard
* Early-stopping
"""

# Commented out IPython magic to ensure Python compatibility.
# Tensorboard

# %load_ext tensorboard

import datetime

# Function to build a TensorBoard callback
def create_tensorboard_callback():
  # Create a log directory for storing TensorBoard logs
  logdir = os.path.join("drive/My Drive/Dog Vision/logs",
                        # Make it so the logs get tracked whenever we run an experiment
                        datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  return tf.keras.callbacks.TensorBoard(logdir)

# Early stopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss",
                                                  patience=5)

"""#### Training a model"""

NUM_EPOCHS = 100 #@param {type:"slider", min:100, max:1500, step:50}

# Building a function to train and return a trained model
def train_model(model, train_data, val_data, NUM_EPOCHES = NUM_EPOCHS):
  """
  Trains a given model and returns the trained version.
  """
  # Create a model
  # model = build_model(model)

  # Create new TensorBoard session everytime we train a model
  tensorboard = create_tensorboard_callback()

  # Fit the model to the data passing it the callbacks we created
  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard, early_stopping])
  # Return the fitted model
  return model

model_1 = train_model(INCEPTION_V3, train_data, val_data)
model_2 = train_model(RESENT_V2, train_data, val_data)
model_3 = train_model(INCEPTION_RESENT_V2, train_data, val_data)

# Commented out IPython magic to ensure Python compatibility.
# Checking the tensorboard
# %tensorboard --logdir drive/My\ Drive/Dog\ Vision/logs

"""#### Saving and reloading a trained model

* Saving using .h5 extensions for tensorflow models
* Reloading using tensorflow API
"""

# function to save a model
def save_model(model, suffix=None):
  """
  Saves a given model in a models directory and appends a suffix (string).
  """
  # Create a model directory pathname with current time
  modeldir = os.path.join("drive/My Drive/Dog Vision/models",
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
  model_path = modeldir + "-" + suffix + ".h5" # save format of model
  print(f"Saving model to: {model_path}...")
  model.save(model_path)
  return model_path

# function to load a trained model
def load_model(model_path):
  """
  Loads a saved model from a specified path.
  """
  print(f"Loading saved model from: {model_path}")
  model = tf.keras.models.load_model(model_path, 
                                     custom_objects={"KerasLayer":hub.KerasLayer})
  return model

INCEPTION_RESNET_VER2 = load_model("/content/drive/MyDrive/Dog Vision/models/20201216-19151608146141-full-image-INCEPTION_RESENT_V2-Adam-L2-ver2.h5")

predictions = INCEPTION_RESNET_VER2.predict(val_data)

"""#### Evaluating the model predictions

**Unbatching the data**
"""

# function to unbatch a batch dataset
def unbatchifying(data):
  """
  Input: Batched dataset of (image, label) Tensors and 
  Output: Reutrns separate arrays of images and labels.
  """
  images = []
  labels = []
  # Loop through unbatched data
  for image, label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(breeds[np.argmax(label)])
  return images, labels

# Unbatchify the validation data
val_images, val_labels = unbatchifying(val_data)
val_images[0], val_labels[0]

"""Visualizing the prediction and truth of validation data """

# Turn prediction probabilities into their respective label (easier to understand)
def get_pred_label(prediction_probabilities):
  """
  Turns an array of prediction probabilities into a label.
  """
  return breeds[np.argmax(prediction_probabilities)]

import matplotlib.pyplot as plt

def plot_pred(prediction_probabilities, labels, images, n=1):
  """
  View the prediction, ground truth and image for sample n
  """
  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]

  # Get the pred label
  pred_label = get_pred_label(pred_prob)

  # Plot image & remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  # Change the colour of the title depending on if the prediction is right or wrong
  if pred_label == true_label:
    color = "green"
  else:
    color = "red"
  
  # Change plot title to be predicted, probability of prediction and truth label
  plt.title("{} {:2.0f}% {}".format(pred_label,
                                    np.max(pred_prob)*100,
                                    true_label),
                                    color=color)

plot_pred(prediction_probabilities=predictions,
          labels=val_labels,
          images=val_images,
          n=88)

"""Function to view the model's top 10 predictions.


* Take an input of prediction probabilities array and a ground truth array and an integer 
* Find the prediction using `get_pred_label()` 
* Find the top 10:
  * Prediction probabilities indexes 
  * Prediction probabilities values 
  * Prediction labels 
* Plot the top 10 prediction probability values and labels, coloring the true label green 
"""

def plot_pred_conf(prediction_probabilities, labels, n=1):
  """
  Plus the top 10 highest prediction confidences along with the truth label for sample n.
  """
  pred_prob, true_label = prediction_probabilities[n], labels[n]

  # Get the predicted label
  pred_label = get_pred_label(pred_prob)

  # Find the top 10 prediction confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]
  # Find the top 10 prediction confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]
  # Find the top 10 prediction labels
  top_10_pred_labels = breeds[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                     top_10_pred_values,
                     color="grey")
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation="vertical")
  
  # Green color for true label
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")
  else:
    pass

# Few predictions and their different values
i_multiplier = 20
num_rows = 3
num_cols = 2
num_images = num_rows*num_cols
plt.figure(figsize=(10*num_cols, 5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(prediction_probabilities=predictions,
            labels=val_labels,
            images=val_images,
            n=i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(prediction_probabilities=predictions,
                 labels=val_labels,
                 n=i+i_multiplier)
plt.tight_layout(h_pad=1.0)
plt.show()

"""#### Training on complete dataset"""

len(X), len(y)

complete_data = get_data_batches(X, y)

complete_model = build_model(MODEL_URL_2)

# Creating full model callbacks
complete_model_tensorboard = create_tensorboard_callback()
# Monitoring cost function
complete_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor="loss",
                                                             patience=4)

# Fit the full model to the full data

INCEPTION_RESENT_V2.fit(x = complete_data,
                        epochs = 250,
                        callbacks=[complete_model_tensorboard, complete_model_early_stopping]
)

save_model(INCEPTION_RESENT_V2, suffix="full-image-INCEPTION_RESENT_V2-Adam-L2-ver2")

loaded_model = load_model('/content/drive/MyDrive/Dog Vision/models/20201216-09271608110871-full-image-INCEPTION_RESENT_V2-Adam-L2.h5')

"""#### Making predictions on test dataset provided by Kaggle"""

# Load test image filenames
test_path = "drive/My Drive/Dog Vision/test/"
test_filenames = [test_path + fname for fname in os.listdir(test_path)]

# Creating test data batch
test_data = get_data_batches(test_filenames, test_set=True)

# Preds

test_predictions = loaded_model.predict(test_data,
                                             verbose=1)

# Saving predictions (NumPy array) to csv file (for access later)
np.savetxt("drive/My Drive/Dog Vision/preds_array_v2.csv", test_predictions, delimiter=",")

# Load predictions (NumPy array) from csv file
test_predictions = np.loadtxt("drive/My Drive/Dog Vision/preds_array_v2.csv_v2", delimiter=",")

test_predictions.shape

"""#### Preparing test dataset for Kaggle submission"""

# pandas DataFrame with empty columns
preds_df = pd.DataFrame(columns=["id"] + list(breeds))
preds_df.head()

# Append test image ID's to predictions DataFrame
test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]
preds_df["id"] = test_ids

# Add the prediction probabilities to each dog breed column
preds_df[list(breeds)] = test_predictions
preds_df.head()

# Saving predictions dataframe to CSV for submission to Kaggle
preds_df.to_csv("drive/My Drive/Dog Vision/INCEPTION_RESENT_V2_model_predictions_submission_v2.csv",
                index=False)

predict_dog_breed('/content/drive/MyDrive/Dog Vision/sample_dogs/', loaded_model)